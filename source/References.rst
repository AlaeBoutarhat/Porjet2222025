References
+++++++++++
.. footer::
   :class: rst-footer-buttons

   :doc:`Previous <introduction>` | :doc:`Next <Building>`



1. Kirillov, A., Mintun, E., Ravi, N., et al. (2023). *Segment Anything*. arXiv preprint arXiv:2304.02643. https://arxiv.org/abs/2304.02643

2. Minderer, M., Wang, X., Bochkovskiy, A., et al. (2022). *Simple Open-Vocabulary Object Detection with Vision Transformers*. arXiv preprint arXiv:2205.06230. https://arxiv.org/abs/2205.06230

3. Liu, S., Zhang, Z., Wu, J., et al. (2023). *Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection*. arXiv preprint arXiv:2303.05499. https://arxiv.org/abs/2303.05499

4. Redmon, J., & Farhadi, A. (2018). *YOLOv3: An Incremental Improvement*. arXiv preprint arXiv:1804.02767. https://arxiv.org/abs/1804.02767

5. Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2020). *BERTScore: Evaluating Text Generation with BERT*. International Conference on Learning Representations (ICLR). https://arxiv.org/abs/1904.09675

6. Hugging Face. *Zephyr 7B Beta*. https://huggingface.co/HuggingFaceH4/zephyr-7b-beta

7. RunPod.io. *Serverless GPU Cloud Infrastructure*. https://www.runpod.io/

8. Ollama. *Run open-source large language models locally*. https://ollama.com

9. Hugging Face. *Transformers Documentation*. https://huggingface.co/docs/transformers
